<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" class="logo" width="120"/>

# План улучшения торговой модели на основе статьи «ИИ в крипто-торговле: возможен ли успех? (Часть 1)»

Ни представлен поэтапный план модернизации существующей модели на **stable_baselines3 + PPO**, описанной в статье. План ориентирован на повышение устойчивости результатов, снижение суммы просадок и ускорение выхода в прибыль при торговле фьючерсами на криптовалюты.

## 1. Уточнение цели и метрик

Перед техническими изменениями формализуйте:

- основную метрику (например, геометрическую доходность портфеля),
- допустимые максимальные просадки,
- горизонт тестирования (in-sample / out-of-sample) и частоту ребалансировки.
Чёткая формулировка цели предотвращает «дрейф» гиперпараметров и упрощает сравнение версий модели[^1].


## 2. Качество данных и предобработка

### 2.1 Расширение датасета

Подключите биржи с высокой ликвидностью и разнообразными инструментами (Bybit, Binance, OKX). Используйте **ccxt.async_support** для параллельной загрузки исторических котировок и обновлений по WebSocket, уменьшая задержки ввода данных[^2].

### 2.2 Очистка и синхронизация

1. Фильтрация аномальных свечей и ошибок биржевого API (spikes).
2. Коррекция пустых интервалов методом прямой интерполяции цен и линейной интерполяции объёмов.
3. Унификация часовых поясов в UTC.

## 3. Пересмотр среды Gym

Создайте собственный класс `CryptoFuturesEnv`, отражающий:

- комиссии тейкера/мейкера,
- скользящее проскальзывание, зависящее от объёма сделки,
- раздельный учёт доходности по лонгу и шорту,
- ликвидации при превышении плеча.
Внедрите `VecNormalize` и `VecMonitor` для автоматической нормализации вознаграждений и логирования[^3].


## 4. Оптимизация архитектуры сети

1. Перейдите с `MlpPolicy` (64×2) на **Residual MLP** 128-256-256-128 с skip-соединениями — для лучшего захвата нелинейных связей.
2. Добавьте слой **LayerNorm** после каждой ReLU для стабилизации градиентов.
3. Включите опциональное использование **LSTM**: `policy_kwargs={"use_lstm": True, "lstm_hidden_size": 256}` — полезно для улавливания волатильных паттернов[^4].

## 5. Углублённый тюнинг гиперпараметров

### 5.1 Optuna + pruners

- Cоздайте `optuna.study` с `MedianPruner`, останавливающим неудачные прогоны на ранних итерациях[^5].
- Ключевые пространства поиска: `learning_rate (1e-5…1e-3, log)`, `gamma (0.9…0.9999)`, `clip_range (0.1…0.3)`, `gae_lambda (0.85…0.99)`, `n_steps (512…4096)`.


### 5.2 Динамическая корректировка во время обучения

Для параметров `gamma` и `clip_range` реализуйте методы-сеттеры в пользовательском классе PPO — так обновится и `rollout_buffer` без перезагрузки модели[^6].

## 6. Повышение sample-эффективности

1. Запускайте **8–16 векторизованных сред** (`SubprocVecEnv`) — это ускорит накопление траекторий и уменьшит дисперсию оценок преимущества[^3].
2. Используйте механизм **frame stacking (4 кадра)** для работы со скоростями изменения цены.

## 7. Риск-менеджмент и капитал

- Ограничьте долю капитала на сделку 1–2%; рассчитывайте плечо так, чтобы уровень ликвидации совпадал с заданной стоп-ценой[^7].
- В среде внедрите штраф `penalty := −k × max(0, equity_min − equity_threshold)` для раннего «наказания» за глубокие просадки.


## 8. Расширение алгоритмов

Параллельно обучите **SAC** на тех же данных: офф-полиси-схема часто даёт лучшую sample-эффективность в нестационарных средах. Затем примените **ensemble-blending**: сигнал = `α·PPO + (1−α)·SAC`.

## 9. Непрерывное обучение (online RL)

Организуйте циклическое «утягивание» новых данных каждые N минут и дообучение модели с низким learning_rate. Применяйте **experience replay** с приоритезацией свежих переходов, чтобы агент адаптировался к рыночным сдвигам без «катастрофического забывания»[^8].

## 10. Модуль исполнения сделок

1. Перепишите слой исполнения на **асинхронный CCXT + WebSocket**. Подтверждение ордера не должно блокировать поток расчёта сигналов[^9].
2. Реализуйте политику отмены (order-timeout) < 500 мс: при задержках ликвидируйте висящие лимитные заявки, чтобы избежать неисполненных хвостов.

## 11. Расширенный бэктест и валидация

- Запустите **walk-forward-analysis**: обучать на март-апрель, тестировать на май; затем сдвиг окна[^10].
- Включите стресс-тесты: «flash-crash» 2017, «COVID-обвал» 2020, «FTX-крах» 2022.
- Сравнивайте **Sharpe**, **Calmar**, **долю прибыльных дней** и `max_drawdown`.


## 12. Мониторинг и A/B-деплой

1. Выкатите новую стратегию на 10% капитала, оставив остальное под старой системой (A/B-тест).
2. Снимайте метрики в Grafana/Prometheus: средний PnL, просадку, спред фактического и теоретического исполнения.
3. При ухудшении метрик > 20% автоматически откатите версию.

## 13. Документы и контроль версий

- Каждая модель должна храниться в DVC вместе с датасетом и yaml-файлом гиперпараметров.
- Чистый ML-CI/CD pipeline (GitHub Actions): линт, unit-тест среды, короткий санити-тест PPO (1 k шагов), диплой в Docker-контейнер.


## 14. Дорожная карта внедрения

| Этап | Длительность | Ключевые результаты |
| :-- | :-- | :-- |
| 1. Подготовка данных | 1 неделя | Расширенный датасет, проверенные котировки |
| 2. Новая Gym-среда | 1 неделя | Поддержка комиссий и ликвидаций |
| 3. Optuna-тюнинг | 2 недели | 50–100 трайлов, выбор гиперпараметров |
| 4. Ensemble PPO/SAC | 1 неделя | Базовое смешивание сигналов |
| 5. Онлайн-обучение | 1 неделя | Цикл дообучения каждые 60 мин |
| 6. A/B-тест | 2 недели | Сравнение на реальных счётах |
| 7. Полный релиз | — | По итогам метрик Sharpe / MDD |

## Заключение

Следуя описанному плану, вы переходите от однофазного PPO-бота к гибкой мультиагентной системе с автоматическим тюнингом, продвинутым риск-менеджментом и непрерывным дообучением. Это повышает устойчивость доходности и позволяет быстрее реагировать на «ломкие» события крипторынка, минимизируя человеческий фактор и сохраняя конкурентное преимущество[^1][^7][^3][^6][^2].

<div style="text-align: center">⁂</div>

[^1]: https://habr.com/ru/articles/859588/

[^2]: https://www.reddit.com/r/reinforcementlearning/comments/1gc2z6y/ppo_takes_upper_range_of_actions_compared_to_sac/

[^3]: https://docsbot.ai/prompts/programming/async-crypto-exchange

[^4]: https://github.com/optuna/optuna

[^5]: https://www.cryptohopper.com/ru/blog/backtesting-your-crypto-trading-strategy-11790

[^6]: https://www.packtpub.com/en-nz/product/deep-reinforcement-learning-with-python-second-edition-9781839210686/chapter/deep-reinforcement-learning-with-stable-baselines-16/section/vectorized-environments-ch16lvl1sec38

[^7]: https://wenku.csdn.net/answer/4g0yzz81or

[^8]: https://mudrex.com/learn/crypto-futures-risk-management/

[^9]: https://www.byteplus.com/en/topic/450279

[^10]: https://fundyourfx.com/importance-of-continuous-learning-in-forex-trading/

[^11]: https://cryptomus.com/ru/blog/cryptocurrency-trading-strategies-for-beginners

[^12]: https://www.reddit.com/r/reinforcementlearning/comments/1hbt64c/how_to_dynamically_modify_hyperparameters_during/

[^13]: https://www.datacamp.com/tutorial/optuna

[^14]: https://www.coinbase.com/learn/tips-and-tutorials/how-to-backtest-a-crypto-trading-strategy

[^15]: https://arxiv.org/html/2412.20138v3

[^16]: https://sky.pro/wiki/profession/chastye-oshibki-novichkov-v-kriptotrejdinge-i-kak-ih-izbezhat/

[^17]: https://stackoverflow.com/questions/76181453/hyperparameter-tuning-for-custom-gym-env-in-stable-baselines3-using-rl-zoo

[^18]: https://github.com/optuna/optuna-examples/blob/main/rl/sb3_simple.py

[^19]: https://www.cryptohopper.com/ru/blog/11270-using-the-percentage-price-oscillator-ppo-efficiently

[^20]: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html

[^21]: https://www.osl.com/hk-en/academy/article/how-to-backtest-a-crypto-trading-strategy

[^22]: https://www.quantifiedstrategies.com/continuous-learning-in-trading/

[^23]: https://gerchik.com/journal/fondovye-rynki/prop-trejdingovye-kompanii-argumenty-za-i-protiv/

[^24]: https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html

[^25]: https://optuna.org

[^26]: https://bitcoin.tax/blog/backtesting-crypto-trading-strategies/

[^27]: https://www.ibm.com/think/topics/ai-agent-learning

[^28]: https://stable-baselines3.readthedocs.io/en/v1.0/guide/vec_envs.html

[^29]: https://hackmd.io/@mashiroissohandsome/H1CVCtsHh

[^30]: https://www.coinbase.com/learn/futures/how-to-avoid-common-pitfalls-when-trading-crypto-derivatives

[^31]: https://stackoverflow.com/questions/79271818/how-to-dynamically-modify-hyperparameters-during-training-in-stable-baselines-3

[^32]: https://coinsbench.com/asyncio-and-speed-testing-crypto-exchanges-apis-via-ccxt-7b5f167e71d7

[^33]: https://yourstory.com/2025/06/mastering-risk-crypto-bitcoin-ethereum-futures-trading

[^34]: https://subscription.packtpub.com/book/programming/9781839210686/16/ch16lvl1sec38/vectorized-environments

[^35]: https://coinsbench.com/asyncio-and-speed-testing-crypto-exchanges-apis-via-ccxt-7b5f167e71d7?gi=e2cdc0541492

[^36]: https://www.kucoin.com/learn/trading/mastering-risk-management-in-crypto-trading

[^37]: https://stackoverflow.com/questions/75509729/stable-baselines-3-default-parameters

[^38]: https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/vec_envs.rst

[^39]: https://github.com/ccxt/ccxt/blob/master/examples/py/async.py

[^40]: https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-how-it-works-reinforcement-learning-algorithm.html

